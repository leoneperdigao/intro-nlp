{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Lesson 2, Activity 3: Topic clustering algorithm\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar \\\n",
    "(edited: Nadejda Roubtsova, February 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- Implement a clustering algorithm and apply it to the set of posts from the `20 Newsgroups` dataset as specified in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data loading\n",
    "\n",
    "First, let's import the libraries that we are going to use in this notebook. Then, let's define a method to load *training* and *test* subsets using a predefined list of categories. Note that following options are also available:\n",
    "- you can use `load_dataset('all', categories)` to load the whole dataset\n",
    "- you can use `load_dataset('train', None)` to load the set of all topics\n",
    "\n",
    "Note that you are working with the same dataset as last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(a_set, cats):\n",
    "    dataset = fetch_20newsgroups(subset=a_set, categories=cats,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          shuffle=True)\n",
    "    return dataset\n",
    "\n",
    "categories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\"]\n",
    "categories += [\"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"sci.space\", \"talk.politics.mideast\"]\n",
    "\n",
    "newsgroups_train = load_dataset('train', categories)\n",
    "newsgroups_test = load_dataset('test', categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data preprocessing\n",
    "\n",
    "Now let's prepare the data for unsupervised approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "9850 posts in 10 categories\n",
      "\n",
      "Labels: \n",
      "[2, 6, 1, 9, 0, 5, 1, 2, 9, 0]\n",
      "Assumed number of clusters: 10\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "all_news = list(zip(newsgroups_train.data, newsgroups_train.target))\n",
    "all_news += list(zip(newsgroups_test.data, newsgroups_test.target))\n",
    "random.shuffle(all_news)\n",
    "\n",
    "all_news_data = [text for (text, label) in all_news]\n",
    "all_news_labels = [label for (text, label) in all_news]\n",
    "\n",
    "print(\"Data:\")\n",
    "print(str(len(all_news_data)) + \" posts in \"\n",
    "      + str(np.unique(all_news_labels).shape[0]) + \" categories\\n\")\n",
    "\n",
    "print(\"Labels: \")\n",
    "print(all_news_labels[:10])\n",
    "num_clusters = np.unique(all_news_labels).shape[0]\n",
    "print(\"Assumed number of clusters: \" + str(num_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original dimensionality of the data is prohibitively large to allow for efficient clustering, let's reduce its dimensionality using [`Singular Value Decomposition`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data contains: 9850 with 33976 features =>\n",
      "Reduced data contains: 9850 with 300 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.5,\n",
    "                             stop_words='english',\n",
    "                             use_idf=True)\n",
    "\n",
    "def transform(data, vectorizer, dimensions):\n",
    "    trans_data = vectorizer.fit_transform(data)\n",
    "    print(\"Transformed data contains: \" + str(trans_data.shape[0]) +\n",
    "          \" with \" + str(trans_data.shape[1]) + \" features =>\")\n",
    "\n",
    "    #See more examples here:\n",
    "    #https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py\n",
    "    svd = TruncatedSVD(dimensions)\n",
    "    pipe = make_pipeline(svd, Normalizer(copy=False))\n",
    "    reduced_data = pipe.fit_transform(trans_data)\n",
    "\n",
    "    return reduced_data, svd\n",
    "\n",
    "reduced_data, svd = transform(all_news_data, vectorizer, 300)\n",
    "print(\"Reduced data contains: \" + str(reduced_data.shape[0]) +\n",
    "        \" with \" + str(reduced_data.shape[1]) + \" features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply k-means clustering\n",
    "\n",
    "Now, let's cluster the data using [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster(data, num_clusters):\n",
    "    km = KMeans(n_clusters=num_clusters, init='k-means++', \n",
    "                max_iter=100, random_state=0)\n",
    "    km.fit(data)\n",
    "    return km\n",
    "\n",
    "km = cluster(reduced_data, num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the results\n",
    "\n",
    "Finally, let's evaluate the results. See the material from Lesson 3 to get more insights about how to interpret the results. What do the informative words suggest about each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering report:\n",
      "\n",
      "* Homogeneity: 0.44704612364219637\n",
      "* Completeness: 0.48937733354001\n",
      "* V-measure: 0.46725493318104117\n",
      "\n",
      "Most discriminative words per cluster:\n",
      "Cluster 0: \n",
      "don people know think just like time good right ve use doctor does way say make things years medical long really problem want did disease said cause thing going msg probably work read better sure food pain day doesn ll patients didn person cancer lot help believe case little new \n",
      "\n",
      "Cluster 1: \n",
      "car bike engine cars just like new miles ride good don rear ve oil know ford road speed think really drive time right riding dealer used bikes driving got make honda does gear problem power tires way buy little wheel manual clutch want auto turn thing need left year brake \n",
      "\n",
      "Cluster 2: \n",
      "israel jews israeli armenian arab jewish people armenians turkish arabs war muslims muslim killed said state genocide palestinian peace palestinians government did world just armenia turks rights turkey israelis population soldiers like human land anti children fact 000 right soviet villages greek don serbs nazi russian lebanon bosnian palestine think \n",
      "\n",
      "Cluster 3: \n",
      "just edu like does com good sure new got think right did say make ll want dod ve use really know probably article time heard way actually list stuff used let going tell thought years better bmw looking read day look need david believe deleted oh little didn yes isn \n",
      "\n",
      "Cluster 4: \n",
      "space orbit launch shuttle nasa moon earth mission lunar solar satellite hst spacecraft like cost program station just time use think data mars years low science long satellites dc idea new project sun know sky missions need don high rocket sci technology power things mass orbital money propulsion large gov \n",
      "\n",
      "Cluster 5: \n",
      "thanks mail know advance email send hi address does information info list looking edu interested like help mailing com appreciate anybody ve reply tell post copy appreciated use new responses net hello don kind wondering need thank good replies just read program group heard question time request x11r5 used source \n",
      "\n",
      "Cluster 6: \n",
      "game team games year hockey players season play think baseball win good don player teams league nhl time like espn did hit just fans better best really series know years played night playoffs detroit going runs playing pens goal boston toronto ll great won pitching leafs way 10 bad braves \n",
      "\n",
      "Cluster 7: \n",
      "key chip clipper encryption government keys escrow nsa algorithm use des phone security secure public law people crypto encrypted privacy secret data just don bit enforcement chips phones number using used know message wiretap agencies think scheme serial rsa trust make way like court private fbi does time pgp police \n",
      "\n",
      "Cluster 8: \n",
      "window server motif using widget use display application file windows program running code sun set problem x11r5 xterm does color like x11 help run mit version error manager work openwindows screen want know lib user include thanks client files way hi ve available widgets usr just colormap xlib xdm source \n",
      "\n",
      "Cluster 9: \n",
      "sale 00 offer shipping condition asking new drive sell price interested email 10 card original excellent edu used mail 50 25 best 20 cd monitor brand includes meg software box 15 obo following make manuals disk included modem old disks like model great manual power 40 printer 30 hard ram \n",
      "\n",
      "\n",
      "Categories:\n",
      "* comp.windows.x\n",
      "* misc.forsale\n",
      "* rec.autos\n",
      "* rec.motorcycles\n",
      "* rec.sport.baseball\n",
      "* rec.sport.hockey\n",
      "* sci.crypt\n",
      "* sci.med\n",
      "* sci.space\n",
      "* talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(km, labels, svd):\n",
    "    print(\"Clustering report:\\n\")\n",
    "    print(f\"* Homogeneity: {str(metrics.homogeneity_score(labels, km.labels_))}\")\n",
    "    print(f\"* Completeness: {str(metrics.completeness_score(labels, km.labels_))}\")\n",
    "    print(f\"* V-measure: {str(metrics.v_measure_score(labels, km.labels_))}\")\n",
    "\n",
    "    print(\"\\nMost discriminative words per cluster:\")\n",
    "    original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for i in range(num_clusters):\n",
    "        print(\"Cluster \" + str(i) + \": \")\n",
    "        cl_terms = \"\"\n",
    "        for ind in order_centroids[i, :50]:\n",
    "            cl_terms += terms[ind] + \" \"\n",
    "        print(cl_terms + \"\\n\")\n",
    "        \n",
    "evaluate(km, all_news_labels, svd)\n",
    "\n",
    "print(\"\\nCategories:\")\n",
    "for i, category in enumerate(newsgroups_train.target_names):\n",
    "    print(\"*\", category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
