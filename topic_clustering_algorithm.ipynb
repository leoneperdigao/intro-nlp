{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5, Lesson 2, Activity 3: Topic clustering algorithm\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar \\\n",
    "(edited: Nadejda Roubtsova, February 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- Implement a clustering algorithm and apply it to the set of posts from the `20 Newsgroups` dataset as specified in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data loading\n",
    "\n",
    "First, let's import the libraries that we are going to use in this notebook. Then, let's define a method to load *training* and *test* subsets using a predefined list of categories. Note that following options are also available:\n",
    "- you can use `load_dataset('all', categories)` to load the whole dataset\n",
    "- you can use `load_dataset('train', None)` to load the set of all topics\n",
    "\n",
    "Note that you are working with the same dataset as last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(a_set, cats):\n",
    "    dataset = fetch_20newsgroups(subset=a_set, categories=cats,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          shuffle=True)\n",
    "    return dataset\n",
    "\n",
    "categories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\"]\n",
    "categories += [\"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"sci.space\", \"talk.politics.mideast\"]\n",
    "\n",
    "newsgroups_train = load_dataset(# load the training dataset 'train' with the selected categories, as before\n",
    "                                )\n",
    "newsgroups_test = load_dataset(# load the training dataset 'test' with the selected categories, as before\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data preprocessing\n",
    "\n",
    "Now let's prepare the data for unsupervised approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "all_news = list(zip(newsgroups_train.data, newsgroups_train.target))\n",
    "all_news += list(zip(# similarly, add the data and target labels from the test set\n",
    "                     ))\n",
    "random.shuffle(all_news)\n",
    "\n",
    "all_news_data = [text for (text, label) in all_news]\n",
    "all_news_labels = [# similar to above, add labels here\n",
    "                   ]\n",
    "\n",
    "print(\"Data:\")\n",
    "print(str(len(all_news_data)) + \" posts in \"\n",
    "      + str(np.unique(all_news_labels).shape[0]) + \" categories\\n\")\n",
    "\n",
    "print(\"Labels: \")\n",
    "print(# print the first 10 labels\n",
    "      )\n",
    "num_clusters = np.unique(all_news_labels).shape[0]\n",
    "print(\"Assumed number of clusters: \" + str(num_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original dimensionality of the data is prohibitively large to allow for efficient clustering, let's reduce its dimensionality using [`Singular Value Decomposition`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.5,\n",
    "                             stop_words='english',\n",
    "                             use_idf=True)\n",
    "\n",
    "def transform(data, vectorizer, dimensions):\n",
    "    trans_data = vectorizer.fit_transform(data)\n",
    "    print(\"Transformed data contains: \" + str(trans_data.shape[0]) +\n",
    "          \" with \" + str(# return the number of columns\n",
    "                         ) + \" features =>\")\n",
    "\n",
    "    #See more examples here:\n",
    "    #https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py\n",
    "    svd = TruncatedSVD(dimensions)\n",
    "    pipe = make_pipeline(svd, Normalizer(copy=False))\n",
    "    reduced_data = # apply .fit_transform method to pipe, passing in trans_data as an argument\n",
    "\n",
    "    return reduced_data, svd\n",
    "\n",
    "reduced_data, svd = transform(all_news_data, vectorizer, 300)\n",
    "print(\"Reduced data contains: \" + str(reduced_data.shape[0]) +\n",
    "        \" with \" + str(reduced_data.shape[1]) + \" features\") # this should tell you that reduced_data contains 300 \"features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply k-means clustering\n",
    "\n",
    "Now, let's cluster the data using [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster(data, num_clusters):\n",
    "    km = KMeans(n_clusters=num_clusters, init='k-means++', \n",
    "                max_iter=100, random_state=0)\n",
    "    km.fit(data)\n",
    "    return km\n",
    "\n",
    "km = cluster(# apply to the relevant data structures\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the results\n",
    "\n",
    "Finally, let's evaluate the results. See the material from Lesson 3 to get more insights about how to interpret the results. What do the informative words suggest about each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(km, labels, svd):\n",
    "    print(\"Clustering report:\\n\")\n",
    "    print(f\"* Homogeneity: {str(metrics.homogeneity_score(labels, km.labels_))}\")\n",
    "    print(f\"* Completeness: {str(# print out completeness_score\n",
    "                                 )}\")\n",
    "    print(f\"* V-measure: {str(# print out v_measure_score\n",
    "                              )}\")\n",
    "\n",
    "    print(\"\\nMost discriminative words per cluster:\")\n",
    "    original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for i in range(num_clusters):\n",
    "        print(\"Cluster \" + str(i) + \": \")\n",
    "        cl_terms = \"\"\n",
    "        for ind in order_centroids[i, :50]:\n",
    "            cl_terms += terms[ind] + \" \"\n",
    "        print(cl_terms + \"\\n\")\n",
    "        \n",
    "evaluate(# apply to the relevant data structures\n",
    "         )\n",
    "\n",
    "print(\"\\nCategories:\")\n",
    "for i, category in enumerate(newsgroups_train.target_names):\n",
    "    print(\"*\", category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
