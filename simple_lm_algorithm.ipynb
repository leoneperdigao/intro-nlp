{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6, Lesson 4, Activity 6: Simple LM algorithm\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar \\\n",
    "(revised: Nadejda Roubtsova, June 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- Implement and evaluate a simple language model using the data referenced in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define a Language Model class\n",
    "\n",
    "Let's start by importing the libraries and building a Language Model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from numpy import cumsum, sum, searchsorted\n",
    "from numpy.random import rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects of class `LanguageModel` should be able to:\n",
    "\n",
    "* _estimate_ the transition probabilities from the context of given order (i.e., length of context) to the next characters based on some training text;\n",
    "* _predict_ the next character based on any new context;\n",
    "* _generate_ a whole sequence using its predictions.\n",
    "\n",
    "To this end let's add three public methods to the class `LanguageModel` that will provide all the functionality described above:\n",
    "\n",
    "* `train` to estimate the probabilities;\n",
    "* `predict` to predict next character;\n",
    "* `generate` to generate a whole sequence of characters.\n",
    "\n",
    "In the code below:\n",
    "1. The method `train` learns the transition probabilities from a text, which is represented as a string and provided to the method as an argument `sequence`.\n",
    "2. The `transitions` dictionary keeps the number of times the context of the specified order (length) is followed by the current character.\n",
    "3. The method `predict` chooses the most probable character given the preceding one(s). The preceding one(s) are provided to the method as an argument `symbol`.\n",
    "4. If the length of the provided sequence of the previous characters doesn't match the language model order, report an error.\n",
    "5. Return the character with a given probability.\n",
    "6. The method `generate` allows you to generate a sequence of a specified number (`n`) of characters.\n",
    "7. For that, it calls on the `predict` method providing it with the context `start`.\n",
    "8. It moves the context character by character specified number of `n` times, thus allowing you to generate `n` new characters.\n",
    "9. Method `weighted_pick` provides search functionality for the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    def __init__(self, order=1):\n",
    "        '''Initializes a language model of the given order.'''\n",
    "        self._transitions = defaultdict(int)\n",
    "        self._order = order\n",
    "        \n",
    "    def train(self, sequence):\n",
    "        '''Trains the model using sequence.'''\n",
    "        self._symbols = list(set(sequence))\n",
    "        for i in range(len(sequence)-self._order):\n",
    "            self._transitions[sequence[i:i+self._order], sequence[i+self._order]] += 1\n",
    "\n",
    "    def predict(self, symbol):\n",
    "        '''Takes as input a string and predicts the next character.'''\n",
    "        if len(symbol) != self._order:\n",
    "            raise ValueError('Expected string of %d chars, got %d' % (self._order, len(symbol)))\n",
    "        probs = [self._transitions[(symbol, s)] for s in self._symbols]\n",
    "        return self._symbols[self._weighted_pick(probs)]\n",
    "\n",
    "    def generate(self, start, n):\n",
    "        '''Generates n characters from start.'''\n",
    "        result = start\n",
    "        for i in range(n):\n",
    "            new = self.predict(start)\n",
    "            result += new\n",
    "            start = start[1:] + new\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def _weighted_pick(weights):\n",
    "        '''Weighted random selection returns n_picks random indexes.\n",
    "        The chance to pick the index i is given by weights[i].'''\n",
    "        return searchsorted(cumsum(weights), rand()*sum(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use your Language Model in practice\n",
    "\n",
    "Let's try to generate texts using famous books to train the language model. In that case, you should expect to get the generated text that is quite similar in style to the text of the book you trained your language model on.\n",
    "\n",
    "Let's import [urllib](https://docs.python.org/3.0/library/urllib.request.html) that will allow you to access texts from online resources. You can download a book from a collection of [Gutenberg Project books](https://www.gutenberg.org) available online. Let's start with *Robinson Crusoe*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "651508\n",
      "ï»¿The Project Gutenberg eBook of The Life and Adventures of Robinson Crusoe,\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "in_text = \"\"\n",
    "with urlopen('https://www.gutenberg.org/files/521/521-0.txt') as response:\n",
    "    for line in response:\n",
    "        line = line.decode('utf-8')  # Decoding the binary data to text.\n",
    "        in_text += line\n",
    "        \n",
    "print(type(in_text))\n",
    "print(len(in_text))\n",
    "print(in_text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code downloads Robinson Crusoe and puts all the contents of this book in a (very long) string.\n",
    "Let's now train your language model and generate new text, for example, using a context of 4 previous characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your are that, we went to the as in all my strait, if he we fowling from thing the proving my little of \n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(order=4)\n",
    "model.train(in_text)\n",
    "print (model.generate('your', 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with examples of texts of your own choice. E.g., here's Shakespeare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young as all rest but love, that was mine ancient vaulty heavy burthen lies\r\n",
      "    The Foundation, anyone\r\n",
      "provided\r\n",
      "that arise;\r\n",
      "    Dry sorrow craves as thine ear.\r\n",
      "    For thou will doom!\r\n",
      "      Examine of us, look to hear more by crossing fantasy;\r\n",
      "    Being sharp sauce.\r\n",
      "\r\n",
      "  Abr. Do not make the widest variance may call him only love, pronounce it fain;\r\n",
      "    One painted bow of lath,\r\n",
      "    A gentlemen!\r\n",
      "    Come hither.\r\n",
      "\r\n",
      "  Abr. Quarrel withdraw; but young, I pray thee, young as all run,\r\n",
      "    How now, my hearts!- You provide a replacement shall happiness thou didst thou art deceiv'd! I would with or appears, or both your cousin?\r\n",
      "    That manner of a pretty piece of Cats, nothing\r\n",
      "    By her found,\r\n",
      "               and Gregory, on my humour, not Romeo's name speak the dearest me; for I will,\r\n",
      "    And light- more inexorable states who\r\n",
      "agree tops-\r\n",
      "\r\n",
      "  Friar Laurence's doom?\r\n",
      "\r\n",
      "  Ben. Here comes\r\n",
      "    Is Rosaline and there.\r\n",
      "\r\n",
      "  Friar, the doors of a beauties; or, if he helps not, let me see \n"
     ]
    }
   ],
   "source": [
    "book = 'http://www.gutenberg.org/cache/epub/1112/pg1112.txt' # Romeo and Juliet\n",
    "in_text = \"\"\n",
    "with urlopen(book) as response:\n",
    "    for line in response:\n",
    "        line = line.decode('utf-8')\n",
    "        in_text += line\n",
    "\n",
    "        \n",
    "model = LanguageModel(order=6)\n",
    "model.train(in_text)\n",
    "print (model.generate('young ', 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate your Language Model\n",
    "\n",
    "The most widely used measure of the language model performance is *perplexity*, which measures how probable in the language the piece of text generated by a language model is. Let's implement this measure and evaluate the text generated by the language models above.\n",
    "\n",
    "The first step is to collect the data which you will use to calculate probabilities. You can use some text from before as the data for perplexity estimation. \n",
    "\n",
    "The data contains strings of symbols (characters) and before calculating the probabilities of words, you need to tokenise it into constituent words. Let's use `spaCy` for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40687\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "tokens = nlp(in_text)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate unigram (word-based) probabilities in this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram(tokens):    \n",
    "    model = defaultdict(lambda: 0.001)\n",
    "    for token in tokens:\n",
    "        token = str(token).strip()\n",
    "        model[token] += 1\n",
    "        \n",
    "    total = 0\n",
    "    for word in model:\n",
    "        total+= model.get(word)\n",
    "    for word in model:\n",
    "        model[word] = model.get(word)/float(total)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's estimate perplexity using the formula from the lecrtures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(testset, model):\n",
    "    testset = nlp(testset)\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[str(word).strip()])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that a good language model will generate a highly probable sequence of words. Recall from the lecture that lower perplexity values signify better models. When you use perplexity to measure the quality of and to compare several language models, you are looking for the one that has **lower perplexity**.\n",
    "\n",
    "With all the components in place, let's apply this measurement and compare a language model that generates text using previous $4$ characters with the one that uses previous $6$ characters. The more likely the words generated by the language model are, the lower the perplexity score. Which model is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be gone onlinks are now?\r\n",
      "\r\n",
      "  Romeo?\r\n",
      "\r\n",
      "\r\n",
      "    That things did.\r\n",
      "\r\n",
      "     How not at here, bride.\r\n",
      "\r\n",
      "    And torches which was Mercutions to do be to Romeo.\r\n",
      "\r\n",
      "    Should quited States to make he will,\r\n",
      "  Nurse, a bawd, and paper.\r\n",
      "\r\n",
      "    That in the farewell, I cannot swift to cooks,\n",
      "223.03026910262616\n",
      "\n",
      "to be taken.- Stay not budge for cost.\r\n",
      "    Come, bitterly.\r\n",
      "    These dear according voice.\r\n",
      "                     Enter Romeo's body\r\n",
      "    Is Romeo, will he comes from her,\r\n",
      "    Alike bewitched puling forth the moon,\r\n",
      "    I drew to Laurence] and alone.\r\n",
      "\r\n",
      "  Prince of my ears ere it is.\n",
      "367.7318790170735\n"
     ]
    }
   ],
   "source": [
    "lm_data = in_text\n",
    "model = unigram(tokens)\n",
    "\n",
    "lm1 = LanguageModel(order=4)\n",
    "lm1.train(lm_data)\n",
    "testset1 = lm1.generate('to b', 280)\n",
    "print(testset1)\n",
    "print(perplexity(testset1, model))\n",
    "print()\n",
    "\n",
    "lm2 = LanguageModel(order=6)\n",
    "lm2.train(lm_data)\n",
    "testset2 = lm2.generate('to be ', 280)\n",
    "print(testset2)\n",
    "print(perplexity(testset2, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
