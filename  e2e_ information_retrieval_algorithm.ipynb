{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Lesson 4, Activity 9: End-to-end IR application\n",
    "\n",
    "&copy;2021, Ekaterina Kochmar \\\n",
    "(edited: Nadejda Roubtsova, June 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this activity is to:\n",
    "\n",
    "- To implement all the steps discussed in the lecture and apply your IR algorithm to the collection of documents and the set of queries provided with this notebook.\n",
    "\n",
    "Note that we continue working on the same data, so we will rely on the implementation of the previous steps (Steps 1–3) from Lesson 2.3. In this notebook, you need to start with Step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in the data (same as before)\n",
    "\n",
    "There are three components to this data to be read in:\n",
    "- documents with their ids and content – there are $1460$ of those to be precise;\n",
    "- questions / queries with their ids and content – there are $112$ of those;\n",
    "- mapping between the queries and relevant documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460 documents in total\n",
      "Document with id 1:\n",
      " 18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n"
     ]
    }
   ],
   "source": [
    "def read_documents():\n",
    "    f = open(\"cisi/CISI.ALL\")\n",
    "    merged = \"\"\n",
    "    \n",
    "    for a_line in f.readlines():\n",
    "        if a_line.startswith(\".\"):\n",
    "            merged += \"\\n\" + a_line.strip()\n",
    "        else:\n",
    "            merged += \" \" + a_line.strip()\n",
    "    \n",
    "    documents = {}\n",
    "\n",
    "    content = \"\"\n",
    "    doc_id = \"\"\n",
    "\n",
    "    for a_line in merged.split(\"\\n\"):\n",
    "        if a_line.startswith(\".I\"):\n",
    "            doc_id = a_line.split(\" \")[1].strip()\n",
    "        elif a_line.startswith(\".X\"):\n",
    "            documents[doc_id] = content\n",
    "            content = \"\"\n",
    "            doc_id = \"\"\n",
    "        else:\n",
    "            content += a_line.strip()[3:] + \" \"\n",
    "    f.close()\n",
    "    return documents\n",
    "\n",
    "documents = read_documents()\n",
    "print(f\"{len(documents)} documents in total\")\n",
    "print(\"Document with id 1:\")\n",
    "print(documents.get(\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's read in queries from the `CISI.QRY` file and store the result in the `queries` data structure where query contents are stored under corresponding query ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 queries in total\n",
      "Query with id 1:\n",
      "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n"
     ]
    }
   ],
   "source": [
    "def read_queries():\n",
    "    f = open(\"cisi/CISI.QRY\")\n",
    "    merged = \"\"\n",
    "    \n",
    "    for a_line in f.readlines():\n",
    "        if a_line.startswith(\".\"):\n",
    "            merged += \"\\n\" + a_line.strip()\n",
    "        else:\n",
    "            merged += \" \" + a_line.strip()\n",
    "    \n",
    "    queries = {}\n",
    "\n",
    "    content = \"\"\n",
    "    qry_id = \"\"\n",
    "\n",
    "    for a_line in merged.split(\"\\n\"):\n",
    "        if a_line.startswith(\".I\"):\n",
    "            if not content==\"\":\n",
    "                queries[qry_id] = content\n",
    "                content = \"\"\n",
    "                qry_id = \"\"\n",
    "            qry_id = a_line.split(\" \")[1].strip()\n",
    "        elif a_line.startswith(\".W\") or a_line.startswith(\".T\"):\n",
    "            content += a_line.strip()[3:] + \" \"\n",
    "    queries[qry_id] = content\n",
    "    f.close()\n",
    "    return queries\n",
    "\n",
    "queries = read_queries()\n",
    "print(f\"{len(queries)} queries in total\")\n",
    "print(\"Query with id 1:\")\n",
    "print(queries.get(\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's read in the mapping between the queries and the documents. We'll keep these in the `mappings` data structure where each query index (key) corresponds to the list of one or more document indices (value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 mappings in total\n",
      "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '37', '39', '41', '42', '43', '44', '45', '46', '49', '50', '52', '54', '55', '56', '57', '58', '61', '62', '65', '66', '67', '69', '71', '76', '79', '81', '82', '84', '90', '92', '95', '96', '97', '98', '99', '100', '101', '102', '104', '109', '111'])\n",
      "Mapping for query with id 1:\n",
      "['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"
     ]
    }
   ],
   "source": [
    "def read_mappings():\n",
    "    f = open(\"cisi/CISI.REL\")\n",
    "    \n",
    "    mappings = {}\n",
    "\n",
    "    for a_line in f.readlines():\n",
    "        voc = a_line.strip().split()\n",
    "        key = voc[0].strip()\n",
    "        current_value = voc[1].strip()\n",
    "        value = []\n",
    "        if key in mappings.keys():\n",
    "            value = mappings.get(key)\n",
    "        value.append(current_value)\n",
    "        mappings[key] = value\n",
    "\n",
    "    f.close()\n",
    "    return mappings\n",
    "\n",
    "mappings = read_mappings()\n",
    "print(f\"{len(mappings)} mappings in total\")\n",
    "print(mappings.keys())\n",
    "print(\"Mapping for query with id 1:\")\n",
    "print(mappings.get(\"1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the data (same as before)\n",
    "\n",
    "Practise application of the following steps:\n",
    "- tokenize the texts\n",
    "- put all to lowercase\n",
    "- remove stopwords\n",
    "- apply stemming\n",
    "\n",
    "Implement and apply these steps to a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cost', 'analys', 'sim', 'proc', 'evalu', 'larg', 'inform', 'system', 'bourn', 'c.p', 'ford', 'd.f', 'comput', 'program', 'writ', 'us', 'sim', 'several-year', 'op', 'inform', 'system', 'comput', 'estim', 'expect', 'op', 'cost', 'wel', 'amount', 'equip', 'personnel', 'requir', 'tim', 'period', 'program', 'us', 'analys', 'sev', 'larg', 'system', 'prov', 'us', 'research', 'tool', 'study', 'system', 'many', 'compon', 'interrel', 'op', 'equ', 'man', 'analys', 'would', 'extrem', 'cumbersom', 'tim', 'consum', 'perhap', 'ev', 'impract', 'pap', 'describ', 'program', 'show', 'exampl', 'result', 'sim', 'two', 'sev', 'suggest', 'design', 'spec', 'inform', 'system']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def process(text): \n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    st = LancasterStemmer()\n",
    "    word_list = [st.stem(word) for word in word_tokenize(text.lower())\n",
    "                 if not word in stoplist and not word in string.punctuation]\n",
    "    return word_list\n",
    "  \n",
    "word_list = process(documents.get(\"27\"))\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Term weighting (same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460 documents in total\n",
      "Terms and frequencies for document with id 1:\n",
      "{'18': 1, 'edit': 4, 'dewey': 3, 'decim': 2, 'class': 2, 'comarom': 1, 'j.p.': 1, 'pres': 1, 'study': 1, 'hist': 2, 'first': 2, 'ddc': 2, 'publ': 1, '1876': 1, 'eighteen': 1, '1971': 1, 'fut': 1, 'continu': 1, 'appear': 1, 'nee': 1, 'spit': 1, \"'s\": 1, 'long': 1, 'healthy': 1, 'lif': 1, 'howev': 1, 'ful': 1, 'story': 1, 'nev': 1, 'told': 1, 'biograph': 1, 'brief': 1, 'describ': 1, 'system': 1, 'attempt': 1, 'provid': 1, 'detail': 1, 'work': 1, 'spur': 1, 'grow': 1, 'libr': 1, 'country': 1, 'abroad': 1}\n",
      "43 terms in this document\n",
      "\n",
      "112 queries in total\n",
      "Terms and frequencies for query with id 1:\n",
      "{'problem': 1, 'concern': 1, 'mak': 1, 'describ': 1, 'titl': 3, 'difficul': 1, 'involv': 1, 'autom': 1, 'retriev': 1, 'artic': 2, 'approxim': 1, 'us': 1, 'relev': 1, 'cont': 1}\n",
      "14 terms in this query\n"
     ]
    }
   ],
   "source": [
    "def get_terms(text): \n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    terms = {}\n",
    "    st = LancasterStemmer()\n",
    "    word_list = [st.stem(word) for word in word_tokenize(text.lower())\n",
    "                 if not word in stoplist and not word in string.punctuation]\n",
    "    for word in word_list:\n",
    "        terms[word] = terms.get(word, 0) + 1\n",
    "    return terms\n",
    "\n",
    "doc_terms = {}\n",
    "qry_terms = {}\n",
    "for doc_id in documents.keys():\n",
    "    doc_terms[doc_id] = get_terms(documents.get(doc_id))\n",
    "for qry_id in queries.keys():\n",
    "    qry_terms[qry_id] = get_terms(queries.get(qry_id))\n",
    "\n",
    "\n",
    "print(f\"{len(doc_terms)} documents in total\")\n",
    "d1_terms = doc_terms.get(\"1\")\n",
    "print(\"Terms and frequencies for document with id 1:\")\n",
    "print(d1_terms)\n",
    "print(f\"{len(d1_terms)} terms in this document\")\n",
    "print()\n",
    "print(f\"{len(qry_terms)} queries in total\")\n",
    "q1_terms = qry_terms.get(\"1\")\n",
    "print(\"Terms and frequencies for query with id 1:\")\n",
    "print(q1_terms)\n",
    "print(f\"{len(q1_terms)} terms in this query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, collect shared vocabulary from all documents and queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7775 terms in the shared vocabulary\n",
      "First 10:\n",
      "[\"''\", \"'60\", \"'70\", \"'anyhow\", \"'apparent\", \"'basic\", \"'better\", \"'bibliograph\", \"'bibliometrics\", \"'building\"]\n"
     ]
    }
   ],
   "source": [
    "def collect_vocabulary():\n",
    "    all_terms = []\n",
    "    for doc_id in doc_terms.keys():\n",
    "        for term in doc_terms.get(doc_id).keys():            \n",
    "            all_terms.append(term)\n",
    "    for qry_id in qry_terms.keys():\n",
    "        for term in qry_terms.get(qry_id).keys():\n",
    "            all_terms.append(term)\n",
    "    return sorted(set(all_terms))\n",
    "\n",
    "all_terms = collect_vocabulary()\n",
    "print(f\"{len(all_terms)} terms in the shared vocabulary\")\n",
    "print(\"First 10:\")\n",
    "print(all_terms[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent each document and query as vectors containing word counts in the shared space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460 document vectors\n",
      "7775 terms in this document\n",
      "112 query vectors\n",
      "7775 terms in this query\n"
     ]
    }
   ],
   "source": [
    "def vectorize(input_terms, shared_vocabulary):\n",
    "    output = {}\n",
    "    for item_id in input_terms.keys():\n",
    "        terms = input_terms.get(item_id)\n",
    "        output_vector = []\n",
    "        for word in shared_vocabulary:\n",
    "            if word in terms.keys():\n",
    "                output_vector.append(int(terms.get(word)))\n",
    "            else:\n",
    "                output_vector.append(0)\n",
    "        output[item_id] = output_vector\n",
    "    return output\n",
    "\n",
    "doc_vectors = vectorize(doc_terms, all_terms)\n",
    "qry_vectors = vectorize(qry_terms, all_terms)\n",
    "\n",
    "print(f\"{len(doc_vectors)} document vectors\")\n",
    "d1460_vector = doc_vectors.get(\"1460\")\n",
    "print(f\"{len(d1460_vector)} terms in this document\")\n",
    "print(f\"{len(qry_vectors)} query vectors\")\n",
    "q112_vector = qry_vectors.get(\"112\")\n",
    "print(f\"{len(q112_vector)} terms in this query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrieval of the most similar documents\n",
    "\n",
    "Use cosine similarity on a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9701425001453319\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "query = [1, 1]\n",
    "document = [3, 5]\n",
    "\n",
    "def length(vector):\n",
    "    sq_length = 0\n",
    "    for index in range(0, len(vector)):\n",
    "        sq_length += math.pow(vector[index], 2)\n",
    "    return math.sqrt(sq_length)\n",
    "    \n",
    "def dot_product(vector1, vector2):\n",
    "    if len(vector1)==len(vector2):\n",
    "        dot_prod = 0\n",
    "        for index in range(0, len(vector1)):\n",
    "            if not vector1[index]==0 and not vector2[index]==0:\n",
    "                dot_prod += vector1[index]*vector2[index]\n",
    "        return dot_prod\n",
    "    else:\n",
    "        return \"Unmatching dimensionality\"\n",
    "\n",
    "def calculate_cosine(query, document):\n",
    "    cosine =  dot_product(query, document) / (length(query) * length(document)) \n",
    "    return cosine\n",
    "\n",
    "cosine = calculate_cosine(query, document)\n",
    "print (cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get cosine similarity for some examples of a particular query and a particular document from the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4830458915396479\n"
     ]
    }
   ],
   "source": [
    "document = doc_vectors.get(\"60\")\n",
    "query = qry_vectors.get(\"3\")\n",
    "\n",
    "cosine =  dot_product(query, document) / (length(query) * length(document))     \n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document and query weighting schemes:**\n",
    "In the implementation of Lesson 2.3 (if you copied all the cells from Step 3), you may have noticed that the documents are not weighted in the same way as the queries. This is not uncommon. More information on the practice and different weighting schemes can be found in: \n",
    "Christopher D. Manning  *et.al*, *Introduction to Information Retrieval*, Cambridge University Press. 2008. \\\n",
    "https://nlp.stanford.edu/IR-book/pdf/06vect.pdf (Section 6.4.3) \\\n",
    "Please feel free to experiment with alternative weighting schemes for query and document vectors as part of these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the search algorithm to find relevant documents for a particular query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 469 with similarity 0.6107\n",
      "Doc 1179 with similarity 0.5643\n",
      "Doc 372 with similarity 0.4899\n",
      "Doc 1116 with similarity 0.4862\n",
      "Doc 1190 with similarity 0.4845\n",
      "Doc 60 with similarity 0.483\n",
      "Doc 803 with similarity 0.4781\n",
      "Doc 1181 with similarity 0.4603\n",
      "Doc 599 with similarity 0.4554\n",
      "Doc 241 with similarity 0.4526\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "results = {}\n",
    "\n",
    "for doc_id in doc_vectors.keys():\n",
    "    document = doc_vectors.get(doc_id)\n",
    "    cosine = calculate_cosine(query, document)    \n",
    "    results[doc_id] = cosine\n",
    "\n",
    "# sort the results dictionary by cosine similarity in descending order and get the top 10\n",
    "sorted_results = sorted(results.items(), key=itemgetter(1), reverse=True)[:10]\n",
    "\n",
    "for items in sorted_results:  # print out the top 10 most similar documents according to cosine similarity\n",
    "    print(f\"Doc {items[0]} with similarity {round(items[1], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation\n",
    "\n",
    "**Optional**: Implement evaluation metrics from Lesson 5 (see Activity 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
